{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7765UFHoyGx6"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "KVtTDrUNyL7x"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0_fqL3ayLHX"
   },
   "source": [
    "# Gradient Boosted Trees: Model understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PS6_yKSoyLAl"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/estimators/boosted_trees_model_understanding\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/estimators/boosted_trees_model_understanding.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/tree/master/site/en/tutorials/estimators/boosted_trees_model_understanding.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dW3r7qVxzqN5"
   },
   "source": [
    "\n",
    "For an end-to-end walkthrough of training a Gradient Boosting model check out the [boosted trees tutorial](https://www.tensorflow.org/tutorials/boosted_trees). In this tutorial you will:\n",
    "\n",
    "* Learn how to interpret a Boosted Tree model both *locally* and *globally*\n",
    "* Gain intution for how a Boosted Trees model fits a dataset\n",
    "\n",
    "## How to interpret Boosted Trees models both locally and globally\n",
    "\n",
    "Local interpretability refers to an understanding of a modelâ€™s predictions at the individual example level, while global interpretability refers to an understanding of the model as a whole. Such techniques can help machine learning (ML) practitioners detect bias and bugs during the model development stage\n",
    "\n",
    "For local interpretability, you will learn how to create and visualize per-instance contributions. To distinguish this from feature importances, we refer to these values as directional feature contributions (DFCs).\n",
    "\n",
    "For global interpretability you will retrieve and visualize gain-based feature importances, [permutation feature importances](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) and also show aggregated DFCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eylrTPAN3rJV"
   },
   "source": [
    "## Load the titanic dataset\n",
    "You will be using the titanic dataset, where the (rather morbid) goal is to predict passenger survival, given characteristics such as gender, age, class, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuhAiPfZ3rJW"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Load dataset.\n",
    "dftrain = pd.read_csv('https://storage.googleapis.com/tfbt/titanic_train.csv')\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tfbt/titanic_eval.csv')\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ioodHdVJVdA"
   },
   "source": [
    "For a description of the features, please review the prior tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krkRHuMp3rJn"
   },
   "source": [
    "## Create feature columns, input_fn, and the train the estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JiJ6K3hr1lXW"
   },
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udMytRJC05oW"
   },
   "source": [
    "Create the feature columns, using the original numeric columns as is and one-hot-encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upaNWxcF3rJn"
   },
   "outputs": [],
   "source": [
    "fc = tf.feature_column\n",
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
    "                       'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "def one_hot_cat_column(feature_name, vocab):\n",
    "  return fc.indicator_column(\n",
    "      fc.categorical_column_with_vocabulary_list(feature_name,\n",
    "                                                 vocab))\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "  # Need to one-hot encode categorical features.\n",
    "  vocabulary = dftrain[feature_name].unique()\n",
    "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "  feature_columns.append(fc.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rTefnXe1n0v"
   },
   "source": [
    "### Build the input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UOlROp33rJo"
   },
   "source": [
    "Create the input functions using the `from_tensor_slices` method in the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API to read in data directly from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dquwCQB3rJp"
   },
   "outputs": [],
   "source": [
    "# Use entire batch since this is such a small dataset.\n",
    "NUM_EXAMPLES = len(y_train)\n",
    "\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
    "  def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))\n",
    "    if shuffle:\n",
    "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "    # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "    dataset = (dataset\n",
    "      .repeat(n_epochs)\n",
    "      .batch(NUM_EXAMPLES))\n",
    "    return dataset\n",
    "  return input_fn\n",
    "\n",
    "# Training and evaluation input functions.\n",
    "train_input_fn = make_input_fn(dftrain, y_train)\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HttfNNlN3rJr"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "tgEzMtlw3rJu",
    "outputId": "36ab0d4a-5dbd-4adb-f724-595b6becb2e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.806818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.864738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.845967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.422891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.422891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.760870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.384993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.707071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "accuracy                0.806818\n",
       "accuracy_baseline       0.625000\n",
       "auc                     0.864738\n",
       "auc_precision_recall    0.845967\n",
       "average_loss            0.422891\n",
       "label/mean              0.375000\n",
       "loss                    0.422891\n",
       "precision               0.760870\n",
       "prediction/mean         0.384993\n",
       "recall                  0.707071\n",
       "global_step           100.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "  'n_trees': 50,\n",
    "  'max_depth': 3,\n",
    "  'n_batches_per_layer': 1,\n",
    "  # You must enable center_bias = True to get DFCs. This will force the model to\n",
    "  # make an initial prediction before using any features (e.g. use the mean of\n",
    "  # the training labels for regression or log odds for classification when\n",
    "  # using cross entropy loss).\n",
    "  'center_bias': True\n",
    "}\n",
    "\n",
    "est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n",
    "est.train(train_input_fn, max_steps=100)\n",
    "results = est.evaluate(eval_input_fn)\n",
    "pd.Series(results).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUrakbu6sqKe"
   },
   "source": [
    "For performance reasons, when your data fits in memory, we recommend use the `boosted_trees_classifier_train_in_memory` function. However if training time is not of a concern or if you have a very large dataset and want to do distributed training, use the `tf.estimator.BoostedTrees` API shown above.\n",
    "\n",
    "\n",
    "When using this method, you should not batch your input data, as the method operates on the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "-4_xz3b_D0W5",
    "outputId": "f890446b-9bec-4ca7-f14f-eeb5bdb8e092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value boosted_trees/are_features_cached\n\t [[node boosted_trees/are_features_cached/read (defined at C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py:158) ]]\n\nCaused by op 'boosted_trees/are_features_cached/read', defined at:\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\asyncio\\base_events.py\", line 528, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1764, in _run_once\n    handle._run()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-8ad9815d66ea>\", line 14, in <module>\n    **in_memory_params)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\contrib\\estimator\\python\\estimator\\boosted_trees.py\", line 303, in boosted_trees_classifier_train_in_memory\n    hooks=train_hooks)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 358, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1124, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1154, in _train_model_default\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1112, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\contrib\\estimator\\python\\estimator\\boosted_trees.py\", line 289, in _model_fn\n    train_in_memory=True)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 982, in _bt_model_fn\n    are_boundaries_ready)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 329, in _cache_transformed_features\n    are_features_cached = _local_variable(False, name='are_features_cached')\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 158, in _local_variable\n    name=name)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1557, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3889, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value boosted_trees/are_features_cached\n\t [[node boosted_trees/are_features_cached/read (defined at C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py:158) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value boosted_trees/are_features_cached\n\t [[{{node boosted_trees/are_features_cached/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8ad9815d66ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfeature_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     **in_memory_params)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\contrib\\estimator\\python\\estimator\\boosted_trees.py\u001b[0m in \u001b[0;36mboosted_trees_classifier_train_in_memory\u001b[1;34m(train_input_fn, feature_columns, model_dir, n_classes, weight_column, label_vocabulary, n_trees, max_depth, learning_rate, l1_regularization, l2_regularization, tree_complexity, min_node_weight, config, train_hooks, center_bias, pruning_mode, quantile_sketch_epsilon)\u001b[0m\n\u001b[0;32m    301\u001b[0m   in_memory_classifier.train(\n\u001b[0;32m    302\u001b[0m       \u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_validate_input_fn_and_repeat_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m       hooks=train_hooks)\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0min_memory_classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[0;32m   1157\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m                                              saving_listeners)\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[1;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[0msave_summaries_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m         log_step_count_steps=log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[0;32m   1404\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[1;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[0;32m    506\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    932\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[0;32m    933\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    646\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sess_creator)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \"\"\"\n\u001b[0;32m   1121\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m     \u001b[0m_WrappedSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1128\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    803\u001b[0m       \u001b[1;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m       \u001b[1;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m       \u001b[1;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[1;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0minit_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m     \u001b[0mlocal_init_success\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_run_local_init_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlocal_init_success\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m       raise RuntimeError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\u001b[0m in \u001b[0;36m_try_run_local_init_op\u001b[1;34m(self, sess)\u001b[0m\n\u001b[0;32m    490\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mis_ready_for_local_init\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Running local_init_op.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_init_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_init_run_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done running local_init_op.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value boosted_trees/are_features_cached\n\t [[node boosted_trees/are_features_cached/read (defined at C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py:158) ]]\n\nCaused by op 'boosted_trees/are_features_cached/read', defined at:\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\asyncio\\base_events.py\", line 528, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1764, in _run_once\n    handle._run()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-8ad9815d66ea>\", line 14, in <module>\n    **in_memory_params)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\contrib\\estimator\\python\\estimator\\boosted_trees.py\", line 303, in boosted_trees_classifier_train_in_memory\n    hooks=train_hooks)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 358, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1124, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1154, in _train_model_default\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1112, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\contrib\\estimator\\python\\estimator\\boosted_trees.py\", line 289, in _model_fn\n    train_in_memory=True)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 982, in _bt_model_fn\n    are_boundaries_ready)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 329, in _cache_transformed_features\n    are_features_cached = _local_variable(False, name='are_features_cached')\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 158, in _local_variable\n    name=name)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1557, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3889, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value boosted_trees/are_features_cached\n\t [[node boosted_trees/are_features_cached/read (defined at C:\\Users\\Eugenio\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py:158) ]]\n"
     ]
    }
   ],
   "source": [
    "in_memory_params = dict(params)\n",
    "del in_memory_params['n_batches_per_layer']\n",
    "# In-memory input_fn does not use batching.\n",
    "def make_inmemory_train_input_fn(X, y):\n",
    "  def input_fn():\n",
    "    return dict(X), y\n",
    "  return input_fn\n",
    "train_input_fn = make_inmemory_train_input_fn(dftrain, y_train)\n",
    "\n",
    "# Train the model.\n",
    "est = tf.contrib.estimator.boosted_trees_classifier_train_in_memory(\n",
    "    train_input_fn,\n",
    "    feature_columns,\n",
    "    **in_memory_params)\n",
    "print(est.evaluate(eval_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSZYqNcRuczV"
   },
   "source": [
    "## Model interpretation and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjcfLiI3uczW"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns_colors = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywTtbBvBuczY"
   },
   "source": [
    "## Local interpretability\n",
    "Next you will output the directional feature contributions (DFCs) to explain individual predictions using the approach outlined in [Palczewska et al](https://arxiv.org/pdf/1312.1121.pdf) and by Saabas in [Interpreting Random Forests](http://blog.datadive.net/interpreting-random-forests/) (this method is also available in scikit-learn for Random Forests in the [`treeinterpreter`](https://github.com/andosa/treeinterpreter) package). The DFCs are generated with:\n",
    "\n",
    "`pred_dicts = list(est.experimental_predict_with_explanations(pred_input_fn))`\n",
    "\n",
    "(Note: The method is named experimental as we may modify the API before dropping the experimental prefix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIL93B4sDRqE"
   },
   "outputs": [],
   "source": [
    "pred_dicts = list(est.experimental_predict_with_explanations(eval_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "tDPoRx_ZaY1E",
    "outputId": "e0b78290-5bb8-44ac-8a89-58c77e5dea41"
   },
   "outputs": [],
   "source": [
    "# Create DFC Pandas dataframe.\n",
    "labels = y_eval.values\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "df_dfc = pd.DataFrame([pred['dfc'] for pred in pred_dicts])\n",
    "df_dfc.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUKSaVoraY1C"
   },
   "source": [
    "A nice property of DFCs is that the sum of the contributions + the bias is equal to the prediction for a given example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hd9VuizRaY1H"
   },
   "outputs": [],
   "source": [
    "# Sum of DFCs + bias == probabality.\n",
    "bias = pred_dicts[0]['bias']\n",
    "dfc_prob = df_dfc.sum(axis=1) + bias\n",
    "np.testing.assert_almost_equal(dfc_prob.values,\n",
    "                               probs.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIC7qm1gaY1L"
   },
   "source": [
    "Plot DFCs for an individual passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "P3u971LsuczZ",
    "outputId": "c3083c78-1fe6-4b69-f274-e138c74475ae"
   },
   "outputs": [],
   "source": [
    "# Plot results.\n",
    "ID = 182\n",
    "example = df_dfc.iloc[ID]  # Choose ith example from evaluation set.\n",
    "TOP_N = 8  # View top 8 features.\n",
    "sorted_ix = example.abs().sort_values()[-TOP_N:].index\n",
    "ax = example[sorted_ix].plot(kind='barh', color=sns_colors[3])\n",
    "ax.grid(False, axis='y')\n",
    "\n",
    "ax.set_title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\n",
    "ax.set_xlabel('Contribution to predicted probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4i4mjK66FYg"
   },
   "source": [
    "The larger magnitude contributions have a larger impact on the model's prediction. Negative contributions indicate the feature value for this given example reduced the model's prediction, while positive values contribute an increase in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tx5p4vEhuczg"
   },
   "source": [
    "### Improved plotting\n",
    "Let's make the plot nice by color coding based on the contributions' directionality and add the feature values on figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6z_Tq1Pquczj"
   },
   "outputs": [],
   "source": [
    "# Boilerplate code for plotting :)\n",
    "def _get_color(value):\n",
    "    \"\"\"To make positive DFCs plot green, negative DFCs plot red.\"\"\"\n",
    "    green, red = sns.color_palette()[2:4]\n",
    "    if value >= 0: return green\n",
    "    return red\n",
    "\n",
    "def _add_feature_values(feature_values, ax):\n",
    "    \"\"\"Display feature's values on left of plot.\"\"\"\n",
    "    x_coord = ax.get_xlim()[0]\n",
    "    OFFSET = 0.15\n",
    "    for y_coord, (feat_name, feat_val) in enumerate(feature_values.items()):\n",
    "        t = plt.text(x_coord, y_coord - OFFSET, '{}'.format(feat_val), size=12)\n",
    "        t.set_bbox(dict(facecolor='white', alpha=0.5))\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "    font = FontProperties()\n",
    "    font.set_weight('bold')\n",
    "    t = plt.text(x_coord, y_coord + 1 - OFFSET, 'feature\\nvalue',\n",
    "    fontproperties=font, size=12)\n",
    "\n",
    "def plot_example(example):\n",
    "  TOP_N = 8 # View top 8 features.\n",
    "  sorted_ix = example.abs().sort_values()[-TOP_N:].index  # Sort by magnitude.\n",
    "  example = example[sorted_ix]\n",
    "  colors = example.map(_get_color).tolist()\n",
    "  ax = example.to_frame().plot(kind='barh',\n",
    "                          color=[colors],\n",
    "                          legend=None,\n",
    "                          alpha=0.75,\n",
    "                          figsize=(10,6))\n",
    "  ax.grid(False, axis='y')\n",
    "  ax.set_yticklabels(ax.get_yticklabels(), size=14)\n",
    "\n",
    "  # Add feature values.\n",
    "  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)\n",
    "  return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlrsuOu8-Yds"
   },
   "source": [
    "Plot example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "id": "Ht1P2-1euczk",
    "outputId": "0dfa4052-841a-4afe-e15f-c3333fe012de"
   },
   "outputs": [],
   "source": [
    "example = df_dfc.iloc[ID]  # Choose IDth example from evaluation set.\n",
    "ax = plot_example(example)\n",
    "ax.set_title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\n",
    "ax.set_xlabel('Contribution to predicted probability', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0swvlkZFaY1Z"
   },
   "source": [
    "You can also plot the example's DFCs compare with the entire distribution using a voilin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zo7rNd1v_5e2"
   },
   "outputs": [],
   "source": [
    "# Boilerplate plotting code.\n",
    "def dist_violin_plot(df_dfc, ID):\n",
    "  # Initialize plot.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "  # Create example dataframe.\n",
    "  TOP_N = 8  # View top 8 features.\n",
    "  example = df_dfc.iloc[ID]\n",
    "  ix = example.abs().sort_values()[-TOP_N:].index\n",
    "  example = example[ix]\n",
    "  example_df = example.to_frame(name='dfc')\n",
    "\n",
    "  # Add contributions of entire distribution.\n",
    "  parts=ax.violinplot([df_dfc[w] for w in ix],\n",
    "                 vert=False,\n",
    "                 showextrema=False,\n",
    "                 widths=0.7,\n",
    "                 positions=np.arange(len(ix)))\n",
    "  face_color = sns_colors[0]\n",
    "  alpha = 0.15\n",
    "  for pc in parts['bodies']:\n",
    "      pc.set_facecolor(face_color)\n",
    "      pc.set_alpha(alpha)\n",
    "\n",
    "  # Add feature values.\n",
    "  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)\n",
    "\n",
    "  # Add local contributions.\n",
    "  ax.scatter(example,\n",
    "              np.arange(example.shape[0]),\n",
    "              color=sns.color_palette()[2],\n",
    "              s=100,\n",
    "              marker=\"s\",\n",
    "              label='contributions for example')\n",
    "\n",
    "  # Legend\n",
    "  # Proxy plot, to show violinplot dist on legend.\n",
    "  ax.plot([0,0], [1,1], label='eval set contributions\\ndistributions',\n",
    "          color=face_color, alpha=alpha, linewidth=10)\n",
    "  legend = ax.legend(loc='lower right', shadow=True, fontsize='x-large',\n",
    "                     frameon=True)\n",
    "  legend.get_frame().set_facecolor('white')\n",
    "\n",
    "  # Format plot.\n",
    "  ax.set_yticks(np.arange(example.shape[0]))\n",
    "  ax.set_yticklabels(example.index)\n",
    "  ax.grid(False, axis='y')\n",
    "  ax.set_xlabel('Contribution to predicted probability', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiLw2tlm_9aK"
   },
   "source": [
    "Plot this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "VkCqraA2uczm",
    "outputId": "0f8c9d3a-4b29-43cc-d673-4dba8be6535d"
   },
   "outputs": [],
   "source": [
    "dist_violin_plot(df_dfc, ID)\n",
    "plt.title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVJFM85SAWVq"
   },
   "source": [
    "Finally, third-party tools, such as [LIME](https://github.com/marcotcr/lime) and [shap](https://github.com/slundberg/shap), can also help understand individual predictions for a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnNXH6mZuczr"
   },
   "source": [
    "## Global feature importances\n",
    "\n",
    "Additionally, you might want to understand the model as a whole, rather than studying individual predictions. Below, you will compute and use:\n",
    "\n",
    "1. Gain-based feature importances using `est.experimental_feature_importances`\n",
    "2. Permutation importances\n",
    "3. Aggregate DFCs using `est.experimental_predict_with_explanations`\n",
    "\n",
    "Gain-based feature importances measure the loss change when splitting on a particular feature, while permutation feature importances are computed by evaluating model performance on the evaluation set by shuffling each feature one-by-one and attributing the change in model performance to the shuffled feature.\n",
    "\n",
    "In general, permutation feature importance are preferred to gain-based feature importance, though both methods can be unreliable in situations where potential predictor variables vary in their scale of measurement or their number of categories and when features are correlated ([source](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307)). Check out [this article](http://explained.ai/rf-importance/index.html) for an in-depth overview and great discussion on different feature importance types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ocBcMatuczs"
   },
   "source": [
    "### 1. Gain-based feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMaxCgPbBJ-j"
   },
   "source": [
    "Gain-based feature importances are built into the TensorFlow Boosted Trees estimators using `est.experimental_feature_importances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "pPTxbAaeuczt",
    "outputId": "297911c6-92b8-46d4-aa8e-fa821820d1f5"
   },
   "outputs": [],
   "source": [
    "importances = est.experimental_feature_importances(normalize=True)\n",
    "df_imp = pd.Series(importances)\n",
    "\n",
    "# Visualize importances.\n",
    "N = 8\n",
    "ax = (df_imp.iloc[0:N][::-1]\n",
    "    .plot(kind='barh',\n",
    "          color=sns_colors[0],\n",
    "          title='Gain feature importances',\n",
    "          figsize=(10, 6)))\n",
    "ax.grid(False, axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GvfAcBeGuczw"
   },
   "source": [
    "### 2. Average absolute DFCs\n",
    "You can also average the absolute values of DFCs to understand impact at a global level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "JkvAWLWLuczx",
    "outputId": "554a836b-2bde-4a6f-e46c-04fe693777a7"
   },
   "outputs": [],
   "source": [
    "# Plot.\n",
    "dfc_mean = df_dfc.abs().mean()\n",
    "N = 8\n",
    "sorted_ix = dfc_mean.abs().sort_values()[-N:].index  # Average and sort by absolute.\n",
    "ax = dfc_mean[sorted_ix].plot(kind='barh',\n",
    "                       color=sns_colors[1],\n",
    "                       title='Mean |directional feature contributions|',\n",
    "                       figsize=(10, 6))\n",
    "ax.grid(False, axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0k_DvPLaY1o"
   },
   "source": [
    "You can also see how DFCs vary as a feature value varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "ZcIfN1IpaY1o",
    "outputId": "7d9185fd-4489-44a7-820f-28ff10a88c58"
   },
   "outputs": [],
   "source": [
    "FEATURE = 'fare'\n",
    "feature = pd.Series(df_dfc[FEATURE].values, index=dfeval[FEATURE].values).sort_index()\n",
    "ax = sns.regplot(feature.index.values, feature.values, lowess=True)\n",
    "ax.set_ylabel('contribution')\n",
    "ax.set_xlabel(FEATURE)\n",
    "ax.set_xlim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbpG72ULucz0"
   },
   "source": [
    "### 3. Permutation feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "6esOw1VOucz0",
    "outputId": "b3ddf27c-7c93-47ad-9169-51e46192d268"
   },
   "outputs": [],
   "source": [
    "def permutation_importances(est, X_eval, y_eval, metric, features):\n",
    "    \"\"\"Column by column, shuffle values and observe effect on eval set.\n",
    "\n",
    "    source: http://explained.ai/rf-importance/index.html\n",
    "    A similar approach can be done during training. See \"Drop-column importance\"\n",
    "    in the above article.\"\"\"\n",
    "    baseline = metric(est, X_eval, y_eval)\n",
    "    imp = []\n",
    "    for col in features:\n",
    "        save = X_eval[col].copy()\n",
    "        X_eval[col] = np.random.permutation(X_eval[col])\n",
    "        m = metric(est, X_eval, y_eval)\n",
    "        X_eval[col] = save\n",
    "        imp.append(baseline - m)\n",
    "    return np.array(imp)\n",
    "\n",
    "def accuracy_metric(est, X, y):\n",
    "    \"\"\"TensorFlow estimator accuracy.\"\"\"\n",
    "    eval_input_fn = make_input_fn(X,\n",
    "                                  y=y,\n",
    "                                  shuffle=False,\n",
    "                                  n_epochs=1)\n",
    "    return est.evaluate(input_fn=eval_input_fn)['accuracy']\n",
    "features = CATEGORICAL_COLUMNS + NUMERIC_COLUMNS\n",
    "importances = permutation_importances(est, dfeval, y_eval, accuracy_metric,\n",
    "                                      features)\n",
    "df_imp = pd.Series(importances, index=features)\n",
    "\n",
    "sorted_ix = df_imp.abs().sort_values().index\n",
    "ax = df_imp[sorted_ix][-5:].plot(kind='barh', color=sns_colors[2], figsize=(10, 6))\n",
    "ax.grid(False, axis='y')\n",
    "ax.set_title('Permutation feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E236y3pVEzHg"
   },
   "source": [
    "# Visualizing model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrcQ-839EzZ6"
   },
   "source": [
    "Lets first simulate/create training data using the following formula:\n",
    "\n",
    "\n",
    "$$z=x* e^{-x^2 - y^2}$$\n",
    "\n",
    "\n",
    "Where \\\\(z\\\\) is the dependent variable you are trying to predict and \\\\(x\\\\) and \\\\(y\\\\) are the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8woaj81GGE9"
   },
   "outputs": [],
   "source": [
    "from numpy.random import uniform, seed\n",
    "from matplotlib.mlab import griddata\n",
    "\n",
    "# Create fake data\n",
    "seed(0)\n",
    "npts = 5000\n",
    "x = uniform(-2, 2, npts)\n",
    "y = uniform(-2, 2, npts)\n",
    "z = x*np.exp(-x**2 - y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRI3KHfLZsGP"
   },
   "outputs": [],
   "source": [
    "# Prep data for training.\n",
    "df = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "\n",
    "xi = np.linspace(-2.0, 2.0, 200),\n",
    "yi = np.linspace(-2.1, 2.1, 210),\n",
    "xi,yi = np.meshgrid(xi, yi)\n",
    "\n",
    "df_predict = pd.DataFrame({\n",
    "    'x' : xi.flatten(),\n",
    "    'y' : yi.flatten(),\n",
    "})\n",
    "predict_shape = xi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0JnH4IhZuAb"
   },
   "outputs": [],
   "source": [
    "def plot_contour(x, y, z, **kwargs):\n",
    "  # Grid the data.\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  # Contour the gridded data, plotting dots at the nonuniform data points.\n",
    "  CS = plt.contour(x, y, z, 15, linewidths=0.5, colors='k')\n",
    "  CS = plt.contourf(x, y, z, 15,\n",
    "                    vmax=abs(zi).max(), vmin=-abs(zi).max(), cmap='RdBu_r')\n",
    "  plt.colorbar()  # Draw colorbar.\n",
    "  # Plot data points.\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KF7WsIcYGF_E"
   },
   "source": [
    "You can visualize the function. Redder colors correspond to larger function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "colab_type": "code",
    "id": "WrxuqaaXGFOK",
    "outputId": "825f68e4-8d37-48ba-a1e4-6f3aa308640b"
   },
   "outputs": [],
   "source": [
    "zi = griddata(x, y, z, xi, yi, interp='linear')\n",
    "plot_contour(xi, yi, zi)\n",
    "plt.scatter(df.x, df.y, marker='.')\n",
    "plt.title('Contour on training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hoANr0f2GFrM"
   },
   "outputs": [],
   "source": [
    "fc = [tf.feature_column.numeric_column('x'),\n",
    "      tf.feature_column.numeric_column('y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVRWyoY3ayTK"
   },
   "outputs": [],
   "source": [
    "def predict(est):\n",
    "  \"\"\"Predictions from a given estimator.\"\"\"\n",
    "  predict_input_fn = lambda: tf.data.Dataset.from_tensors(dict(df_predict))\n",
    "  preds = np.array([p['predictions'][0] for p in est.predict(predict_input_fn)])\n",
    "  return preds.reshape(predict_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyPu5618GU7K"
   },
   "source": [
    "First let's try to fit a linear model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUIV2IVgGVSk"
   },
   "outputs": [],
   "source": [
    "train_input_fn = make_input_fn(df, df.z)\n",
    "est = tf.estimator.LinearRegressor(fc)\n",
    "est.train(train_input_fn, max_steps=500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "_u4WAcCqfbco",
    "outputId": "bb9e7ff1-8af2-4c82-972d-0b1fb838ae82"
   },
   "outputs": [],
   "source": [
    "plot_contour(xi, yi, predict(est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XD_fMAUtSCSa"
   },
   "source": [
    "It's not a very good fit. Next let's try to fit a GBDT model to it and try to understand how the model fits the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ka1GgvqmSCK7"
   },
   "outputs": [],
   "source": [
    "def create_bt_est(n_trees):\n",
    "  return tf.estimator.BoostedTreesRegressor(fc,\n",
    "                                            n_batches_per_layer=1,\n",
    "                                            n_trees=n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3953
    },
    "colab_type": "code",
    "id": "w0s86Kq1R_Fc",
    "outputId": "c7518d9f-74a9-4c73-c8c9-c986262843f6"
   },
   "outputs": [],
   "source": [
    "N_TREES = [1,2,3,4,10,20,50,100]\n",
    "for n in N_TREES:\n",
    "  est = create_bt_est(n)\n",
    "  est.train(train_input_fn, max_steps=500)\n",
    "  plot_contour(xi, yi, predict(est))\n",
    "  plt.text(-1.8, 2.1, '# trees: {}'.format(n), color='w', backgroundcolor='black', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WcZ9fubh1wT"
   },
   "source": [
    "As you increase the number of trees, the model's predictions better approximates the underlying function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMKoEZnCdrsp"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSZUSrjXdw9g"
   },
   "source": [
    "In this tutorial you learned how to interpret Boosted Trees models using directional feature contributions and feature importance techniques. These techniques provide insight into how the features impact a model's predictions. Finally, you also gained intution for how a Boosted Tree model fits a complex function by viewing the decision surface for several models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "boosted_trees_model_understanding.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
